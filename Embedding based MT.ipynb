{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eulvfJWl7ueY"
      },
      "source": [
        "# Lab 1\n",
        "\n",
        "\n",
        "## Part 1: Bilingual dictionary induction and unsupervised embedding-based MT\n",
        "*Note: this task is based on materials from yandexdataschool [NLP course](https://github.com/yandexdataschool/nlp_course/). Feel free to check this awesome course if you wish to dig deeper.*\n",
        "\n",
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Here is the link to my work in colab:\n",
        "### https://colab.research.google.com/drive/1QD0OMjjx2DPRf8lEBL6G0Xe6sbyO2Rm3?usp=sharing"
      ],
      "metadata": {
        "id": "5EQUhOqXQ56n"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV4rIjxa7uei"
      },
      "source": [
        "Even without parallel corpora this system can be good enough (hopefully), in particular for similar languages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idSYq2GU7uew"
      },
      "source": [
        "### Frament of the Swadesh list for some slavic languages\n",
        "\n",
        "The Swadesh list is a lexicostatistical stuff. It's named after American linguist Morris Swadesh and contains basic lexis. This list are used to define subgroupings of languages, its relatedness.\n",
        "\n",
        "So we can see some kind of word invariance for different Slavic languages.\n",
        "\n",
        "\n",
        "| Russian         | Belorussian              | Ukrainian               | Polish             | Czech                         | Bulgarian            |\n",
        "|-----------------|--------------------------|-------------------------|--------------------|-------------------------------|-----------------------|\n",
        "| женщина         | жанчына, кабета, баба    | жінка                   | kobieta            | žena                          | жена                  |\n",
        "| мужчина         | мужчына                  | чоловік, мужчина        | mężczyzna          | muž                           | мъж                   |\n",
        "| человек         | чалавек                  | людина, чоловік         | człowiek           | člověk                        | човек                 |\n",
        "| ребёнок, дитя   | дзіця, дзіцёнак, немаўля | дитина, дитя            | dziecko            | dítě                          | дете                  |\n",
        "| жена            | жонка                    | дружина, жінка          | żona               | žena, manželka, choť          | съпруга, жена         |\n",
        "| муж             | муж, гаспадар            | чоловiк, муж            | mąż                | muž, manžel, choť             | съпруг, мъж           |\n",
        "| мать, мама      | маці, матка              | мати, матір, неня, мама | matka              | matka, máma, 'стар.' mateř    | майка                 |\n",
        "| отец, тятя      | бацька, тата             | батько, тато, татусь    | ojciec             | otec                          | баща, татко           |\n",
        "| много           | шмат, багата             | багато                  | wiele              | mnoho, hodně                  | много                 |\n",
        "| несколько       | некалькі, колькі         | декілька, кілька        | kilka              | několik, pár, trocha          | няколко               |\n",
        "| другой, иной    | іншы                     | інший                   | inny               | druhý, jiný                   | друг                  |\n",
        "| зверь, животное | жывёла, звер, істота     | тварина, звір           | zwierzę            | zvíře                         | животно               |\n",
        "| рыба            | рыба                     | риба                    | ryba               | ryba                          | риба                  |\n",
        "| птица           | птушка                   | птах, птиця             | ptak               | pták                          | птица                 |\n",
        "| собака, пёс     | сабака                   | собака, пес             | pies               | pes                           | куче, пес             |\n",
        "| вошь            | вош                      | воша                    | wesz               | veš                           | въшка                 |\n",
        "| змея, гад       | змяя                     | змія, гад               | wąż                | had                           | змия                  |\n",
        "| червь, червяк   | чарвяк                   | хробак, черв'як         | robak              | červ                          | червей                |\n",
        "| дерево          | дрэва                    | дерево                  | drzewo             | strom, dřevo                  | дърво                 |\n",
        "| лес             | лес                      | ліс                     | las                | les                           | гора, лес             |\n",
        "| палка           | кій, палка               | палиця                  | patyk, pręt, pałka | hůl, klacek, prut, kůl, pálka | палка, пръчка, бастун |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNM3_fjr7ue2"
      },
      "source": [
        "But the context distribution of these languages demonstrates even more invariance. And we can use this fact for our for our purposes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLppwa527ue6"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwGoVhRA7ufP"
      },
      "source": [
        "In this notebook we're going to use pretrained word vectors - FastText (original paper - https://arxiv.org/abs/1607.04606).\n",
        "\n",
        "You can download them from the official [website](https://fasttext.cc/docs/en/crawl-vectors.html). We're going to need embeddings for English and French languages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KV2-MpR-ugq-",
        "outputId": "74890c64-6985-48aa-a756-775c568c7132"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-31 19:33:26--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.173.166.48, 18.173.166.74, 18.173.166.51, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.173.166.48|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1325960915 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘cc.en.300.vec.gz’\n",
            "\n",
            "cc.en.300.vec.gz    100%[===================>]   1.23G   138MB/s    in 10s     \n",
            "\n",
            "2024-03-31 19:33:36 (125 MB/s) - ‘cc.en.300.vec.gz’ saved [1325960915/1325960915]\n",
            "\n",
            "--2024-03-31 19:34:34--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.173.166.48, 18.173.166.74, 18.173.166.31, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.173.166.48|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1287757366 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘cc.fr.300.vec.gz’\n",
            "\n",
            "cc.fr.300.vec.gz    100%[===================>]   1.20G  78.9MB/s    in 11s     \n",
            "\n",
            "2024-03-31 19:34:45 (116 MB/s) - ‘cc.fr.300.vec.gz’ saved [1287757366/1287757366]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -nc https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
        "!gzip -d cc.en.300.vec.gz\n",
        "\n",
        "!wget -nc https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.vec.gz\n",
        "!gzip -d cc.fr.300.vec.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kwg26PKLv88U"
      },
      "source": [
        "After downloading and extracting the vectors, we should be able to load them using the [gensim](https://radimrehurek.com/gensim/) library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "u1JjQv_97ufT"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "en_emb = KeyedVectors.load_word2vec_format(\"cc.en.300.vec\")\n",
        "fr_emb = KeyedVectors.load_word2vec_format(\"cc.fr.300.vec\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sqb_XJhkMyHM"
      },
      "source": [
        "Once you've loaded the vectors, you can use the `KeyedVectors` interface to get word embeddings and/or query most similar words by embedding:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTkXfT0W7ufk",
        "outputId": "6ce6172e-583b-4215-8deb-78a74fd994e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((300,), array([-0.0522,  0.0364, -0.1252,  0.0053,  0.0382], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "august_embedding = en_emb[\"august\"]\n",
        "august_embedding.shape, august_embedding[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQ2kCq-7NQPn",
        "outputId": "ca1e66f7-6923-4dfe-eb39-0fe1876f3470"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('august', 0.9999999403953552),\n",
              " ('september', 0.8252838850021362),\n",
              " ('october', 0.8111193180084229),\n",
              " ('june', 0.8050147891044617),\n",
              " ('july', 0.797055184841156),\n",
              " ('november', 0.788363516330719),\n",
              " ('february', 0.7831973433494568),\n",
              " ('december', 0.7824540138244629),\n",
              " ('january', 0.7743154168128967),\n",
              " ('april', 0.7621643543243408)]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "en_emb.most_similar([august_embedding])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5EcMMI6pxzL"
      },
      "source": [
        "The latter function also allows you to vary the amount of closest words via the `topn` argument:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bi6AF3z0p9Oo",
        "outputId": "5c9b66b0-4b73-4847-956b-37727826938f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('august', 0.9999999403953552),\n",
              " ('september', 0.8252838850021362),\n",
              " ('october', 0.8111193180084229)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "en_emb.most_similar([august_embedding], topn=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw345NRXov4p"
      },
      "source": [
        "Another feature of `KeyedVectors` is that it allows to compute embeddings for multiple words simultaneously:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86OuYeLYow0C",
        "outputId": "4997d2c3-905e-451d-9b34-12b088489a33"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "en_emb[[\"august\", \"september\"]].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uGx5zHXQtfo"
      },
      "source": [
        "Everything above is true for the embeddings for French language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdBA8lcg7ufs",
        "outputId": "a5635a7c-dae2-4899-bce4-408254125797"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('aout', 1.0),\n",
              " ('Aout', 0.8249964118003845),\n",
              " ('juillet', 0.8109882473945618),\n",
              " ('fevrier', 0.8072442412376404),\n",
              " ('septembre', 0.7838520407676697),\n",
              " ('août', 0.779176652431488),\n",
              " ('juin', 0.7692081332206726),\n",
              " ('octobre', 0.7597455382347107),\n",
              " ('decembre', 0.7595790028572083),\n",
              " ('avril', 0.7390779256820679)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "fr_emb.most_similar([fr_emb[\"aout\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1Dkka5uQ37-"
      },
      "source": [
        "However, french and english embeddings were trained independently of each other. This means, that there is no obvious connection between values in embeddings for similar words in French and English:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yJvcKXO7uf0",
        "outputId": "70682b6d-6a39-416b-d567-b45e4ebb0bcd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('2003Pays', 0.23082853853702545),\n",
              " ('Montsoriu', 0.22505579888820648),\n",
              " ('2015Pays', 0.22218400239944458),\n",
              " ('2013Genre', 0.2095685601234436),\n",
              " ('AdiCloud', 0.2018650770187378),\n",
              " ('Bagua', 0.20061466097831726),\n",
              " ('2003Paysans', 0.2001495361328125),\n",
              " ('ValenceLa', 0.2001476287841797),\n",
              " ('Luddites', 0.19998176395893097),\n",
              " ('Guadalquivir', 0.19875513017177582)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "fr_emb.most_similar([en_emb[\"august\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lia_h7W2qL8C"
      },
      "source": [
        "## Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNdYAR1q7uf6"
      },
      "source": [
        "We'll build a simple translator, which will try to predict the french embedding from the english one. For this we'll need a dataset of word pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CXbH86oQRprk"
      },
      "outputs": [],
      "source": [
        "def load_word_pairs(filename):\n",
        "    en_fr_pairs = []\n",
        "    en_vectors = []\n",
        "    fr_vectors = []\n",
        "    with open(filename, \"r\") as inpf:\n",
        "        for line in inpf:\n",
        "            en, fr = line.rstrip().split(\" \")\n",
        "            if en not in en_emb or fr not in fr_emb:\n",
        "                continue\n",
        "            en_fr_pairs.append((en, fr))\n",
        "            en_vectors.append(en_emb[en])\n",
        "            fr_vectors.append(fr_emb[fr])\n",
        "    return en_fr_pairs, np.array(en_vectors), np.array(fr_vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwjYGFE7Ui0N"
      },
      "source": [
        "We will train our model to predict embedding for the french word from embedding of its english counterpart. For this reason we split our train and test data into english and french words and compute corresponding embeddings to obtain `X` (english embeddings) and `y` (french embeddings)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPvHHq7Cc_Oa",
        "outputId": "1c1bbe78-f3d1-41f4-925c-1811454cd1ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-31 20:08:25--  https://raw.githubusercontent.com/girafe-ai/ml-course/23s_nes/homeworks/hw04_umt/en-fr.train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 178608 (174K) [text/plain]\n",
            "Saving to: ‘en-fr.train.txt’\n",
            "\n",
            "\ren-fr.train.txt       0%[                    ]       0  --.-KB/s               \ren-fr.train.txt     100%[===================>] 174.42K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-03-31 20:08:25 (7.12 MB/s) - ‘en-fr.train.txt’ saved [178608/178608]\n",
            "\n",
            "--2024-03-31 20:08:25--  https://raw.githubusercontent.com/girafe-ai/ml-course/23s_nes/homeworks/hw04_umt/en-fr.test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 50509 (49K) [text/plain]\n",
            "Saving to: ‘en-fr.test.txt’\n",
            "\n",
            "en-fr.test.txt      100%[===================>]  49.33K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-03-31 20:08:25 (4.52 MB/s) - ‘en-fr.test.txt’ saved [50509/50509]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O en-fr.train.txt https://raw.githubusercontent.com/girafe-ai/ml-course/23s_nes/homeworks/hw04_umt/en-fr.train.txt\n",
        "!wget -O en-fr.test.txt https://raw.githubusercontent.com/girafe-ai/ml-course/23s_nes/homeworks/hw04_umt/en-fr.test.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "K05ari5nSEcn"
      },
      "outputs": [],
      "source": [
        "en_fr_train, X_train, Y_train = load_word_pairs(\"en-fr.train.txt\")\n",
        "en_fr_test, X_test, Y_test = load_word_pairs(\"en-fr.test.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ithG80uDTYWr",
        "outputId": "b5e3bd86-8a81-4730-8e91-ee7c5dd82dab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('which', 'lesquels'),\n",
              " ('which', 'laquelle'),\n",
              " ('which', 'lequel'),\n",
              " ('also', 'également'),\n",
              " ('also', 'aussi'),\n",
              " ('also', 'egalement'),\n",
              " ('were', 'étaient'),\n",
              " ('but', 'mais'),\n",
              " ('have', 'avoir'),\n",
              " ('have', 'ont'),\n",
              " ('one', 'un')]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "en_fr_train[33:44]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZBBNvpz7ugQ"
      },
      "source": [
        "## Embedding space mapping (0.3 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_Dhk5gL7ugS"
      },
      "source": [
        "Let $x_i \\in \\mathrm{R}^d$ be the distributed representation of word $i$ in the source language, and $y_i \\in \\mathrm{R}^d$ is the vector representation of its translation. Our purpose is to learn such linear transform $W$ that minimizes euclidian distance between $Wx_i$ and $y_i$ for some subset of word embeddings. Thus we can formulate so-called [Procrustes problem](https://en.wikipedia.org/wiki/Orthogonal_Procrustes_problem):\n",
        "\n",
        "$$W^*= \\arg\\min_W \\sum_{i=1}^n\\|Wx_i - y_i\\|_2$$\n",
        "\n",
        "or\n",
        "\n",
        "$$W^*= \\arg\\min_W \\|XW^T - Y\\|_F$$\n",
        "\n",
        "where $\\|\\cdot\\|_F$ denotes Frobenius norm.\n",
        "\n",
        "> **Note:** in second formula, $W$ and $x$ seem to have switched places. This happens because the $X$ matrix is composed of objects $x_i$ in *rows* not *columns*, i.e. it is kind of composed of $x_i^T$. This means that $X \\in \\mathbb{R}^{N \\times D}$, where $N$ is the number of items and $D$ is the embedding dimensionality. The same is true for the $Y$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acOjDdtL7ugY"
      },
      "source": [
        "$W^*= \\arg\\min_W \\sum_{i=1}^n\\|Wx_i - y_i\\|_2$ looks like simple multiple linear regression without bias. The `sklearn` allows you to turn off the bias in `LinearRegression` via the `fit_intercept` argument (in fact they simply call bias the intercept). So let's code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Lb-KN1be7uga",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "a8edbf22-8cae-49c7-e8e4-486e07aae502"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression(fit_intercept=False)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression(fit_intercept=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression(fit_intercept=False)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        "# YOUR CODE HERE\n",
        "mapping = LinearRegression(fit_intercept = False)\n",
        "mapping.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7tqJwoY7ugf"
      },
      "source": [
        "Let's take a look at neigbours of the vector of word _\"august\"_ (_\"aout\"_ in French) after linear transform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31SrFSbn7ugi",
        "outputId": "5001a4b5-9542-4a19-d9fd-338f6542a61d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('juin', 0.7553410530090332),\n",
              " ('aout', 0.7527691721916199),\n",
              " ('juillet', 0.7500795125961304),\n",
              " ('septembre', 0.7482382655143738),\n",
              " ('mars', 0.7415984272956848),\n",
              " ('octobre', 0.7395485043525696),\n",
              " ('novembre', 0.7313361763954163),\n",
              " ('février', 0.7296543717384338),\n",
              " ('janvier', 0.7272254824638367),\n",
              " ('avril', 0.7249919772148132)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "august = mapping.predict(en_emb[\"august\"].reshape(1, -1))\n",
        "fr_emb.most_similar(august)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2uY6Y9B7ugt"
      },
      "source": [
        "As quality measure we will use precision top-1, top-5 and top-10 (for each transformed english embedding we count how many right target pairs are found in top N nearest neighbours in french embedding space)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zptuho8LAfIE"
      },
      "outputs": [],
      "source": [
        "def precision(pairs, mapped_vectors, topn=1):\n",
        "    \"\"\"\n",
        "    :args:\n",
        "        pairs = list of right word pairs [(en_word_0, fr_word_0), ...]\n",
        "        mapped_vectors = list of embeddings after mapping from source embedding space to destination embedding space\n",
        "        topn = the number of nearest neighbours in destination embedding space to choose from\n",
        "    :returns:\n",
        "        precision_val, float number, total number of words for those we can find right translation at top K.\n",
        "    \"\"\"\n",
        "    assert len(pairs) == len(mapped_vectors)\n",
        "    total = len(pairs)\n",
        "    correct = 0\n",
        "    for i in range(total):\n",
        "        pair = pairs[i]\n",
        "        predicted_vector = mapped_vectors[i]\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        emb_list = fr_emb.most_similar(predicted_vector, topn=topn)\n",
        "\n",
        "        for k in range(topn):\n",
        "          if pair[1] == emb_list[k][0]:\n",
        "            correct += 1\n",
        "\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "duhj9hpv7ugy"
      },
      "outputs": [],
      "source": [
        "assert precision([(\"august\", \"aout\")], august, topn=5) == 1.0\n",
        "assert precision([(\"august\", \"aout\")], august, topn=9) == 1.0\n",
        "assert precision([(\"august\", \"aout\")], august, topn=10) == 1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5A9tWtnuFx3"
      },
      "source": [
        "Note that our `precision` function accepts lists of pairs of words, whereas we have dataframes. However, it is not a problem: we can get a list (actually, numpy array) of pairs via the `values` property."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0-iyd5gP7ug5"
      },
      "outputs": [],
      "source": [
        "assert precision(en_fr_test[:100], X_test[:100]) == 0.0\n",
        "assert precision(en_fr_test[:100], Y_test[:100]) == 1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DVV5lqrua_O"
      },
      "source": [
        "Let's see how well our model is doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "U-ssEJ3x7uhA"
      },
      "outputs": [],
      "source": [
        "precision_top1 = precision(en_fr_test[:100], mapping.predict(X_test[:100]), 1)\n",
        "precision_top5 = precision(en_fr_test[:100], mapping.predict(X_test[:100]), 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "JOXKaYj1VHGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff2a6bce-eeb3-4cb8-b568-b88d3df287a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.38\n",
            "0.67\n"
          ]
        }
      ],
      "source": [
        "print(precision_top1)\n",
        "print(precision_top5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf6Ou8bx7uhH"
      },
      "source": [
        "## Making it better (orthogonal Procrustean problem) (0.3 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oLs-drN7uhK"
      },
      "source": [
        "It can be shown that a self-consistent linear mapping between semantic spaces should be orthogonal.\n",
        "We can restrict transform $W$ to be orthogonal. Then we will solve next problem:\n",
        "\n",
        "$$(W^T)^*= \\arg\\min_{W^T} \\|XW^T - Y\\|_F \\text{, where: } W^TW = I$$\n",
        "\n",
        "$$I \\text{- identity matrix}$$\n",
        "\n",
        "Instead of making yet another regression problem we can find optimal orthogonal transformation using singular value decomposition. It turns out that optimal transformation $W^*$ can be expressed via SVD components:\n",
        "$$X^TY=U\\Sigma V^T\\text{, singular value decompostion}$$\n",
        "$$(W^T)^*=UV^T$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "DdFQ7qti7uhL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# YOUR CODE HERE\n",
        "# Compute the orthogonal mapping (W^T)^* as defined in formula above.\n",
        "\n",
        "U, S, Vt = np.linalg.svd(np.matmul(np.transpose(X_train), Y_train), full_matrices=False)\n",
        "mapping_svd = np.matmul(U, Vt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sehLFmlBysc-"
      },
      "source": [
        "Now our `mapping` is just a numpy array, meaning that it has no `predict` method. However, from the formulae above we know, that prediction is done using the matrix multiplication:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "OVOFYYa37uhX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14c98071-f5f4-454e-f20b-fe3eb3b86bdc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('aout', 0.6705766320228577),\n",
              " ('juin', 0.6591026186943054),\n",
              " ('juillet', 0.6516768336296082),\n",
              " ('septembre', 0.6453961133956909),\n",
              " ('octobre', 0.6392979025840759),\n",
              " ('mars', 0.6334785223007202),\n",
              " ('août', 0.6331560611724854),\n",
              " ('février', 0.6244350671768188),\n",
              " ('novembre', 0.6244062185287476),\n",
              " ('avril', 0.6175950765609741)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "fr_emb.most_similar([np.matmul(en_emb['august'], mapping_svd)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4qKCmq7zJDK"
      },
      "source": [
        "Now let's compute our precision values and see, whether our trick did improve the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "r297sYP37uhb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c610dd8b-5c92-425b-b0ab-a33e9223e7ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.36\n",
            "0.68\n"
          ]
        }
      ],
      "source": [
        "print(precision(en_fr_test[:100], np.matmul(X_test[:100], mapping_svd)))\n",
        "print(precision(en_fr_test[:100], np.matmul(X_test[:100], mapping_svd), 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvUZ72U5AfJg"
      },
      "source": [
        "## Unsupervised embedding-based MT (0.4 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLyuVfHBLrJn"
      },
      "source": [
        "Now, let's build our word embeddings-based translator!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa3dAZHv1wjY"
      },
      "source": [
        "Now let's translate these sentences word-by-word. Before that, however, don't forget to tokenize your sentences. For that you may (or may not) find the `nltk.tokenize.WordPunctTokenizer` to be very useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "FGksC7l_NMi9"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "def translate(sentence):\n",
        "    \"\"\"\n",
        "    :args:\n",
        "        sentence - sentence in English (str)\n",
        "    :returns:\n",
        "        translation - sentence in French (str)\n",
        "\n",
        "    * find english embedding for each word in sentence\n",
        "    * transform english embedding vector\n",
        "    * find nearest french word and replace\n",
        "    \"\"\"\n",
        "    translated = []\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    tk = WordPunctTokenizer()\n",
        "\n",
        "    for word in tk.tokenize(sentence):\n",
        "        translated.append(fr_emb.most_similar([np.matmul(en_emb[word], mapping_svd)])[0][0])\n",
        "        #translated.append(fr_emb.most_similar(mapping.predict(en_emb[word].reshape(1, -1)))[0][0])\n",
        "\n",
        "    return \" \".join(translated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "4hbbMy-tNxlf"
      },
      "outputs": [],
      "source": [
        "assert translate(\".\") == \".\"\n",
        "assert translate(\"I walk around Paris\") == \"je marcher autour Paris\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia6I2ce7O_HI"
      },
      "source": [
        "Now you can play with your model and try to get as accurate translations as possible. **Note**: one big issue is out-of-vocabulary words. Try to think of various ways of handling it (you can start with translating each of them to a special **UNK** token and then move to more sophisticated approaches). Good luck!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "17Azt44TW9s3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3fa4168-ac3a-4c09-e22f-d89a83e3b0ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords, twitter_samples\n",
        "import re\n",
        "import string\n",
        "\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def process_tweet(tweet):\n",
        "    '''\n",
        "    Input:\n",
        "        tweet: a string containing a tweet\n",
        "    Output:\n",
        "        tweets_clean: a list of words containing the processed tweet\n",
        "\n",
        "    '''\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    # remove stock market tickers like $GE\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    # remove old style retweet text \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    # remove hyperlinks\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    # tokenize tweets\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        # if (word not in stopwords_english and  # remove stopwords\n",
        "        #     word not in string.punctuation):  # remove punctuation\n",
        "        if word not in string.punctuation:\n",
        "            tweets_clean.append(word)\n",
        "            # stem_word = stemmer.stem(word)  # stemming word\n",
        "            # tweets_clean.append(stem_word)\n",
        "\n",
        "    return \" \".join(tweets_clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nawoCF7kXLyE",
        "outputId": "43fd2e3e-1928-48a7-9b8f-69c517f50af0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['#FollowFriday @wncer1 @Defense_gouv for being top influencers in my community this week :)',\n",
              " \"Who Wouldn't Love These Big....Juicy....Selfies :) - http://t.co/QVzjgd1uFo http://t.co/oWBL11eQRY\",\n",
              " '@Mish23615351  follow @jnlazts &amp; http://t.co/RCvcYYO0Iq follow u back :)',\n",
              " \"@jjulieredburn Perfect, so you already know what's waiting for you :)\",\n",
              " 'Great new opportunity for junior triathletes aged 12 and 13 at the Gatorade series! Get your entries in :) http://t.co/of3DyOzML0']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "twitter_samples.strings('positive_tweets.json')[10:15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XW5avSmX1CD",
        "outputId": "f521cd3a-c0a1-40de-9b4b-a9318af69fc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#FollowFriday @wncer1 @Defense_gouv for being top influencers in my community this week :)\n",
            "\n",
            "followfriday for being top influencers in my community this week :)\n",
            "-----------------\n",
            "Who Wouldn't Love These Big....Juicy....Selfies :) - http://t.co/QVzjgd1uFo http://t.co/oWBL11eQRY\n",
            "\n",
            "who wouldn't love these big ... juicy ... selfies :)\n",
            "-----------------\n",
            "@Mish23615351  follow @jnlazts &amp; http://t.co/RCvcYYO0Iq follow u back :)\n",
            "\n",
            "follow\n",
            "-----------------\n",
            "@jjulieredburn Perfect, so you already know what's waiting for you :)\n",
            "\n",
            "perfect so you already know what's waiting for you :)\n",
            "-----------------\n",
            "Great new opportunity for junior triathletes aged 12 and 13 at the Gatorade series! Get your entries in :) http://t.co/of3DyOzML0\n",
            "\n",
            "great new opportunity for junior triathletes aged 12 and 13 at the gatorade series get your entries in :)\n",
            "-----------------\n"
          ]
        }
      ],
      "source": [
        "for i in twitter_samples.strings('positive_tweets.json')[10:15]:\n",
        "    print(i, process_tweet(i), sep='\\n\\n', end='\\n-----------------\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **My work on making our model better:**\n",
        "\n",
        "Firstly, let us deal with symbols our translator cannot interpret. I wrote a function so we can get rid of emojis and excessive digits (in case we don't do that we see an error, where these ones are perceived as words)."
      ],
      "metadata": {
        "id": "3_2sNkWAYvRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_emojis_and_digits(tweet):\n",
        "  tweet = re.sub(r'[(:)/0-9]', '', tweet)\n",
        "  return tweet"
      ],
      "metadata": {
        "id": "SRd29MmPZpqj"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in twitter_samples.strings('positive_tweets.json')[10:15]:\n",
        "    print(i, remove_emojis_and_digits(process_tweet(i)), sep='\\n\\n', end='\\n-----------------\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vScYh_4LZ-s_",
        "outputId": "0bfaad7c-cbb1-40ab-e5c0-2ef70dbcb756"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#FollowFriday @wncer1 @Defense_gouv for being top influencers in my community this week :)\n",
            "\n",
            "followfriday for being top influencers in my community this week \n",
            "-----------------\n",
            "Who Wouldn't Love These Big....Juicy....Selfies :) - http://t.co/QVzjgd1uFo http://t.co/oWBL11eQRY\n",
            "\n",
            "who wouldn't love these big ... juicy ... selfies \n",
            "-----------------\n",
            "@Mish23615351  follow @jnlazts &amp; http://t.co/RCvcYYO0Iq follow u back :)\n",
            "\n",
            "follow\n",
            "-----------------\n",
            "@jjulieredburn Perfect, so you already know what's waiting for you :)\n",
            "\n",
            "perfect so you already know what's waiting for you \n",
            "-----------------\n",
            "Great new opportunity for junior triathletes aged 12 and 13 at the Gatorade series! Get your entries in :) http://t.co/of3DyOzML0\n",
            "\n",
            "great new opportunity for junior triathletes aged  and  at the gatorade series get your entries in \n",
            "-----------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we want to learn how to handle OOV words. In our environment, we don't have access to the real time search of their meanings and we cannot recover them. And we have plenty of them because our dataset consists of tweets with lots of slang, hashtags and typos, so it is crucial to handle them, the quality of the model is dependent on it highly.\n",
        "\n",
        "We have several options:\n",
        "\n",
        "1. Just ignore them\n",
        "2. Replace them with UNK tokens\n",
        "3. Replace them with 0 vectors\n",
        "4. Replace them with random vectors\n",
        "5. Replace them with synonymous words - but in this case we need a dictionary with their meaning, which we lack by design, as we work with offline translator or the meaning of these words may be non-existent\n",
        "6. Use subword units and n-grams - this is the best way as this makes our model more robust in the face of internet slang and hashtags that can be perfectly interpreted through subwords often\n",
        "\n",
        "However, the best way (6), FastText embeddings prediction, cannot be used because training FastText consumes too much RAM and we cannot do it with the affordable resources (I've been trying to come up with a solution to this problem for several hours and I couldn't fix it anyhow, unfortunately, so we just need to accept that the best performance is not achiavable)\n",
        "\n",
        "However, we can try other ways and check how they influence performance of the model.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fCxfJy1S8-X5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check Droping OOV\n"
      ],
      "metadata": {
        "id": "5-B-UVni1_bd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_dropOOV(sentence):\n",
        "    \"\"\"\n",
        "    :args:\n",
        "        sentence - sentence in English (str)\n",
        "    :returns:\n",
        "        translation - sentence in French (str)\n",
        "\n",
        "    * find english embedding for each word in sentence\n",
        "    * transform english embedding vector\n",
        "    * find nearest french word and replace\n",
        "    \"\"\"\n",
        "    translated = []\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    tk = WordPunctTokenizer()\n",
        "\n",
        "    for word in tk.tokenize(sentence):\n",
        "        try:\n",
        "          translated.append(fr_emb.most_similar([np.matmul(en_emb[word], mapping_svd)])[0][0])\n",
        "          #translated.append(fr_emb.most_similar(mapping.predict(en_emb[word].reshape(1, -1)))[0][0])\n",
        "        except KeyError:\n",
        "          continue\n",
        "\n",
        "    return \" \".join(translated)"
      ],
      "metadata": {
        "id": "_zW_mY3cJeVj"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_tweets = [remove_emojis_and_digits(process_tweet(tweet)) for tweet in twitter_samples.strings('positive_tweets.json')[:10]]"
      ],
      "metadata": {
        "id": "DgP6sOcj2Lw7"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translations_dropOOV = [translate_dropOOV(tweet) for tweet in processed_tweets]"
      ],
      "metadata": {
        "id": "OuDzUWPb2L37"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translations_dropOOV"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5yK6fNHdEV1",
        "outputId": "a9ec76a6-c31f-4ada-c88d-3cdd2e3ec7b4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['twitt pour être top engagé membres dans mon communauté cette semaine',\n",
              " 'hey christopher comment bizarre veuillez appeler notre contacter centre sur et nous pourra être puisse amener aider vous nombreux merci',\n",
              " 'nous avait un écouter dernière nuit comme vous saigner est un incroyable track quand sont vous dans ecosse',\n",
              " 'félicitation',\n",
              " 'hahaha mon cic vérifiée été réussir avais un bleu tique marquer sur mon fb profil dans jours',\n",
              " 'cette autre est irrésistible',\n",
              " 'nous peux \" c veux amener garder notre joli clients attendre pour longue nous espère vous savourer heureux samedi',\n",
              " 'sur deuxième pensé évident c c juste pas suffisamment temps pour un enfnat mais nouveau shorts entrant système brebis doit être acheter',\n",
              " 'mais nous ont amener aller amener bayan R aurevoir',\n",
              " 'comme un acte de espièglerie suis appeler dans ged couche de notre dans ( maison entrepôts appli bomberman bien … comme dans nom sous-entend p']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "t5eoT-uGm_RO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We will measure the precision of our model by built-in nltk f_measure.\n",
        "\n",
        "### Why f_measure, and not precision only, for example? I think that there is a problem, where the reference translation (correct one) will translate one word to two words or more, or interpret a combination of words in a different way. We need to take into account how many of the words of the initial phrase I translated correctly, not only if they appeared in our translated sentence or not. This will help us not to skew our estimation to the higher side (precision does this), so we have a lot of space for improvement.\n",
        "\n",
        "### Also, I don't think it is necessary to use accuracy because we translate sentences word-by-word anyways and join them in this exact order, so they are meant to be in the order by design and it is not the thing we want to estimate.\n",
        "\n",
        "### At first I wanted to measure precision with BLEU, but it works too bad for the word-by-word translation due to the fact that we simply lack n-gramm overlapping (and BLEU score would not work or would be 0 every time), so this metric is not for word-by-word translations.\n",
        "\n",
        "### For references (correct translations) we will pick Bing En-Fr translator, and I will also do a word-by-word translation with it (not sentence-by-sentence), because it would lose sense to compare our MT model with contextual translations (as it would lead to problems with precision estimation, and our model simply cannot translate sentences contextually, and estimation is going to be skewed anyways).\n",
        "\n",
        "### Also, I need to mention that I use Bing Translator and not simple dictionary matches because Bing Translator handles out-of-dictionary words well, so our estimation will be correct as handling out-of-dictionary words is one of the things we want to estimate."
      ],
      "metadata": {
        "id": "mWj9HbERhedp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install translators\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYLauia_ho8c",
        "outputId": "fa88a6d7-ad7b-4777-c61e-cf9f0d97606c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting translators\n",
            "  Downloading translators-5.9.0-py3-none-any.whl (55 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/55.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.29.0 in /usr/local/lib/python3.10/dist-packages (from translators) (2.31.0)\n",
            "Collecting PyExecJS>=1.5.1 (from translators)\n",
            "  Downloading PyExecJS-1.5.1.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from translators) (4.9.4)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from translators) (4.66.2)\n",
            "Collecting pathos>=0.2.9 (from translators)\n",
            "  Downloading pathos-0.3.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cryptography>=42.0.4 in /usr/local/lib/python3.10/dist-packages (from translators) (42.0.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=42.0.4->translators) (1.16.0)\n",
            "Collecting ppft>=1.7.6.8 (from pathos>=0.2.9->translators)\n",
            "  Downloading ppft-1.7.6.8-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill>=0.3.8 (from pathos>=0.2.9->translators)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pox>=0.3.4 (from pathos>=0.2.9->translators)\n",
            "  Downloading pox-0.3.4-py3-none-any.whl (29 kB)\n",
            "Collecting multiprocess>=0.70.16 (from pathos>=0.2.9->translators)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from PyExecJS>=1.5.1->translators) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.29.0->translators) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.29.0->translators) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.29.0->translators) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.29.0->translators) (2024.2.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=42.0.4->translators) (2.21)\n",
            "Building wheels for collected packages: PyExecJS\n",
            "  Building wheel for PyExecJS (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyExecJS: filename=PyExecJS-1.5.1-py3-none-any.whl size=14578 sha256=dac9ae50e326ed410404b822831c7b497c978ff7bb26b1ff3ddf44ead1604607\n",
            "  Stored in directory: /root/.cache/pip/wheels/9d/91/30/28e6da53d4f44dc445349b2ffad581968447e4cbc9dd7991b8\n",
            "Successfully built PyExecJS\n",
            "Installing collected packages: PyExecJS, ppft, pox, dill, multiprocess, pathos, translators\n",
            "Successfully installed PyExecJS-1.5.1 dill-0.3.8 multiprocess-0.70.16 pathos-0.3.2 pox-0.3.4 ppft-1.7.6.8 translators-5.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.metrics.scores import precision, recall, f_measure\n",
        "import translators as ts"
      ],
      "metadata": {
        "id": "USHw6Jrgkcb7"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Bing_translate(sentence):\n",
        "  translated = []\n",
        "  tk = WordPunctTokenizer()\n",
        "  for word in tk.tokenize(sentence):\n",
        "        translated.append(ts.translate_text(word, translator='bing', to_language='fr'))\n",
        "        #translated.append(fr_emb.most_similar(mapping.predict(en_emb[word].reshape(1, -1)))[0][0])\n",
        "  return \" \".join(translated)"
      ],
      "metadata": {
        "id": "lpyB10E_kst1"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bing_translations = [Bing_translate(tweet) for tweet in processed_tweets]"
      ],
      "metadata": {
        "id": "tVogyXY_vHBo"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bing_translations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2Q3NTrW6Er-",
        "outputId": "e1863843-cc64-43da-f1a9-857dbca010fe"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['SuivreVendredi pour être Retour au début fiancé membres dans mon communauté ceci semaine',\n",
              " 'hé James comment odd S’il vous plaît appeler notre contact centre sur et nous volonté être capable À aider vous beaucoup merci',\n",
              " 'nous eu un écouter dernier Nuit comme vous saigner est un stupéfiant piste quand are vous dans Écosse',\n",
              " 'Félicitations',\n",
              " 'Ouais Youpi mon accnt Vérifié RQST (en anglais seulement) a réussir A un bleu tique marque sur mon Fb profil dans Jours',\n",
              " 'ceci un est irrésistible flipkartfashionvendredi',\n",
              " \"nous revêtir ' t comme À garder notre charmant clientèle attente pour long nous espoir vous jouir heureux Vendredi LWWF\",\n",
              " \"sur deuxième pensée là ' s juste non assez Heure pour un Dd mais nouveau short Entrer système mouton devoir être Acheter\",\n",
              " 'L’HGJ mais nous avoir À aller À Le Seigneur du Monde D au revoir',\n",
              " 'comme un acte de Espièglerie suis métier le L’ETL couche de notre dans - maison entreposage appli Catamari puits ... comme le nom Implique p']"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_f_measure(our_translations, references):\n",
        "  total = len(our_translations)\n",
        "  f_measures = []\n",
        "  for i in range(total):\n",
        "    f_measures.append(f_measure(set(references[i].split()), set(our_translations[i].split())))\n",
        "  return np.mean(f_measures)\n"
      ],
      "metadata": {
        "id": "xRbPY6j-w0G2"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_f_measure(translations_dropOOV, bing_translations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJy2-OWsvXM6",
        "outputId": "48606160-ea36-40c8-f04f-5fe80f8330f5"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.414607837214899"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try UNK tokens method:"
      ],
      "metadata": {
        "id": "qP8nLnZULI-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_UNK(sentence):\n",
        "    \"\"\"\n",
        "    :args:\n",
        "        sentence - sentence in English (str)\n",
        "    :returns:\n",
        "        translation - sentence in French (str)\n",
        "\n",
        "    * find english embedding for each word in sentence\n",
        "    * transform english embedding vector\n",
        "    * find nearest french word and replace\n",
        "    \"\"\"\n",
        "    translated = []\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    tk = WordPunctTokenizer()\n",
        "\n",
        "    for word in tk.tokenize(sentence):\n",
        "        try:\n",
        "          translated.append(fr_emb.most_similar([np.matmul(en_emb[word], mapping_svd)])[0][0])\n",
        "          #translated.append(fr_emb.most_similar(mapping.predict(en_emb[word].reshape(1, -1)))[0][0])\n",
        "        except KeyError:\n",
        "          translated.append(fr_emb.most_similar('UNK')[0][0])\n",
        "          continue\n",
        "\n",
        "    return \" \".join(translated)"
      ],
      "metadata": {
        "id": "VWE2SG4VJxkT"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translations_UNK = [translate_UNK(tweet) for tweet in processed_tweets]"
      ],
      "metadata": {
        "id": "ICeYzRhiLP4d"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_f_measure(translations_UNK, bing_translations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7bN8fJeLWqb",
        "outputId": "67eb718d-f100-439a-ec5a-5a1b6ea2e06e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4068732263223934"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So it's actually worse than dropping OOV words. Let's try other methods, that are available to us"
      ],
      "metadata": {
        "id": "lniRoJ3Q7cHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_zerosOOV(sentence):\n",
        "    translated = []\n",
        "\n",
        "    tk = WordPunctTokenizer()\n",
        "\n",
        "    for word in tk.tokenize(sentence):\n",
        "        try:\n",
        "          translated.append(fr_emb.most_similar([np.matmul(en_emb[word], mapping_svd)])[0][0])\n",
        "          #translated.append(fr_emb.most_similar(mapping.predict(en_emb[word].reshape(1, -1)))[0][0])\n",
        "        except KeyError:\n",
        "          translated.append(fr_emb.most_similar([np.matmul(np.zeros((300,)), mapping_svd)])[0][0])\n",
        "          continue\n",
        "\n",
        "    return \" \".join(translated)"
      ],
      "metadata": {
        "id": "QddPa7QZ7ivI"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translations_zerosOOV = [translate_zerosOOV(tweet) for tweet in processed_tweets]"
      ],
      "metadata": {
        "id": "csP3LPU47qu6"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_f_measure(translations_zerosOOV, bing_translations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HetMj99g7-mC",
        "outputId": "1fc5ba91-74a2-4f6f-f52f-bf232ffe7430"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4068732263223934"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Actually, other similar ways won't enhance our model, because this is the way our metric works, our word either correct or not, so all ways to replace OOV words with something incorrect (similar or random) will not work. Just as follows:"
      ],
      "metadata": {
        "id": "e1jTDyT0L6Rx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_randomOOV(sentence):\n",
        "    translated = []\n",
        "\n",
        "    tk = WordPunctTokenizer()\n",
        "\n",
        "    for word in tk.tokenize(sentence):\n",
        "        try:\n",
        "          translated.append(fr_emb.most_similar([np.matmul(en_emb[word], mapping_svd)])[0][0])\n",
        "          #translated.append(fr_emb.most_similar(mapping.predict(en_emb[word].reshape(1, -1)))[0][0])\n",
        "        except KeyError:\n",
        "          translated.append(fr_emb.most_similar([np.matmul(np.random.randn(300), mapping_svd)])[0][0])\n",
        "          continue\n",
        "\n",
        "    return \" \".join(translated)"
      ],
      "metadata": {
        "id": "5hFUXGtK8pGi"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translations_randomOOV = [translate_randomOOV(tweet) for tweet in processed_tweets]"
      ],
      "metadata": {
        "id": "O3OspCtD89u1"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_f_measure(translations_randomOOV, bing_translations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNCl6KDb9CmA",
        "outputId": "5b56cf83-b004-44cf-8c77-af38a4835f90"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.40579360419013966"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So for our metric the best way to handle OOV would be just drop them (as the percentage of correct words would be higher). However, it is not as great for the translation itself. I tried some methods that are somehow similar to FastText approach, where you try to identify whether substring of an OOV is a real word or not:"
      ],
      "metadata": {
        "id": "M_8IMHW2MVWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_firstngramOOV(sentence):\n",
        "    translated = []\n",
        "\n",
        "    tk = WordPunctTokenizer()\n",
        "\n",
        "    for word in tk.tokenize(sentence):\n",
        "        try:\n",
        "          translated.append(fr_emb.most_similar([np.matmul(en_emb[word], mapping_svd)])[0][0])\n",
        "          #translated.append(fr_emb.most_similar(mapping.predict(en_emb[word].reshape(1, -1)))[0][0])\n",
        "        except KeyError:\n",
        "          for i in range(len(word)):\n",
        "            try:\n",
        "              translated.append(fr_emb.most_similar([np.matmul(en_emb[word[:i]], mapping_svd)])[0][0])\n",
        "              break\n",
        "            except KeyError:\n",
        "              continue\n",
        "          continue\n",
        "\n",
        "    return \" \".join(translated)"
      ],
      "metadata": {
        "id": "V67orOzJ9FQG"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translations_firstngramOOV = [translate_firstngramOOV(tweet) for tweet in processed_tweets]"
      ],
      "metadata": {
        "id": "TRpWXh7i-_kM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_f_measure(translations_firstngramOOV, bing_translations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlviXlxA_Zgo",
        "outputId": "160e5aa6-934e-487d-ea67-4f747e29c545"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.40579360419013966"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "But it doesn't work well, as substrings might be incorrect words or not the word user wanted to use. Then, I wanted to see whether OOV words influence the model much, or the quality is dependent on model itself (way of mapping), and I translated OOVs with Bing:"
      ],
      "metadata": {
        "id": "0ABwPTKRMsjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_bingOOV(sentence):\n",
        "    translated = []\n",
        "\n",
        "    tk = WordPunctTokenizer()\n",
        "\n",
        "    for word in tk.tokenize(sentence):\n",
        "        try:\n",
        "          translated.append(fr_emb.most_similar([np.matmul(en_emb[word], mapping_svd)])[0][0])\n",
        "          #translated.append(fr_emb.most_similar(mapping.predict(en_emb[word].reshape(1, -1)))[0][0])\n",
        "        except KeyError:\n",
        "          translated.append(Bing_translate(word))\n",
        "          continue\n",
        "\n",
        "    return \" \".join(translated)"
      ],
      "metadata": {
        "id": "GeuE_789APbC"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translations_bingOOV = [translate_bingOOV(tweet) for tweet in processed_tweets]"
      ],
      "metadata": {
        "id": "93q6k0mvAhNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_f_measure(translations_bingOOV, bing_translations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUdRW8I2AtjS",
        "outputId": "60f07d14-1007-4371-e1b8-4e819887f81d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4610267920549065"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we can see that OOV words do not influence that much, because even when we use translators that can incorporate n-gram methods for translating these words (so we are sure that they are correct), our f measure is still low. So we can focus on other ways to improve the model a bit."
      ],
      "metadata": {
        "id": "B42g_N1vA74t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EBCs6bfqNOLL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, change the way we measure similarity of french embeddings and try cosmul:"
      ],
      "metadata": {
        "id": "0ccDFnYbNH59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_cosmul(sentence):\n",
        "    translated = []\n",
        "\n",
        "    tk = WordPunctTokenizer()\n",
        "\n",
        "    for word in tk.tokenize(sentence):\n",
        "        try:\n",
        "          translated.append(fr_emb.most_similar_cosmul([np.matmul(en_emb[word], mapping_svd)])[0][0])\n",
        "          #translated.append(fr_emb.most_similar(cca.predict(en_emb[word].reshape(1, -1)))[0][0])\n",
        "        except KeyError:\n",
        "          continue\n",
        "\n",
        "    return \" \".join(translated)"
      ],
      "metadata": {
        "id": "BCY4M99jKppJ"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I researched several articles and found classical ways to enhance the performance of an offlinde billingual translator"
      ],
      "metadata": {
        "id": "NDm7XwWjBLzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translations_cosmul = [translate_cosmul(tweet) for tweet in processed_tweets]"
      ],
      "metadata": {
        "id": "ZdSlTCcWKwx_"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_f_measure(translations_cosmul, bing_translations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7v9kTBtqK1LS",
        "outputId": "5425138e-3fbd-4b5b-c036-d446cd0b1f25"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.414607837214899"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well it works but just a bit. I also saw some articles where they use LinReg with l2 regularization to map word-embeddings and decided to try it:"
      ],
      "metadata": {
        "id": "oreNmTKBN3aB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "mapping_Ridge = Ridge()\n",
        "mapping_Ridge.fit(X_train, Y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "1b9pxQD5OD34",
        "outputId": "6b101627-1982-457e-c7e4-93536cc4319e"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ridge()"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Ridge()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_ridge(sentence):\n",
        "    translated = []\n",
        "\n",
        "    tk = WordPunctTokenizer()\n",
        "\n",
        "    for word in tk.tokenize(sentence):\n",
        "        try:\n",
        "          #translated.append(fr_emb.most_similar_cosmul([np.matmul(en_emb[word], mapping_svd)])[0][0])\n",
        "          translated.append(fr_emb.most_similar(mapping_Ridge.predict(en_emb[word].reshape(1, -1)))[0][0])\n",
        "        except KeyError:\n",
        "          continue\n",
        "\n",
        "    return \" \".join(translated)"
      ],
      "metadata": {
        "id": "QQMITWWPORMj"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translations_ridge = [translate_ridge(tweet) for tweet in processed_tweets]"
      ],
      "metadata": {
        "id": "_IWp8cuKOY4v"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_f_measure(translations_ridge, bing_translations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2mZbEYJOdvt",
        "outputId": "0e8e7b0a-391e-4e10-e456-c58f7ef7c4da"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3816963724521864"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Worse than Procrustean"
      ],
      "metadata": {
        "id": "zUtLIyWlOw-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And I also tried mapping with dimensionality reduction, as I also saw it as a way to enhance the model in Tomas Mikolov article"
      ],
      "metadata": {
        "id": "uyLUsaqzOlUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "QjgyGNrSBSKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca_fr = PCA(n_components=100)"
      ],
      "metadata": {
        "id": "cQX2vZOYbE-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca_fr.fit_transform(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GY-mgErGSdv0",
        "outputId": "bea4384b-4355-459b-9b8e-6a82bccc9193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.1009952 ,  0.34358194,  0.09441803, ...,  0.0185993 ,\n",
              "        -0.03974115,  0.07074632],\n",
              "       [-0.10099518,  0.34358105,  0.09441895, ...,  0.01859794,\n",
              "        -0.03974429,  0.07074582],\n",
              "       [-0.10099524,  0.34358194,  0.09441996, ...,  0.0186036 ,\n",
              "        -0.03974203,  0.07074285],\n",
              "       ...,\n",
              "       [ 0.83314604, -0.05229552,  0.16380344, ..., -0.10388121,\n",
              "        -0.14068092,  0.00591609],\n",
              "       [-0.13043436, -0.20010385, -0.01701185, ..., -0.00789266,\n",
              "        -0.0098806 ,  0.01036977],\n",
              "       [-0.13043436, -0.20010385, -0.01701185, ..., -0.0078927 ,\n",
              "        -0.00988061,  0.0103698 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectors_fr = pca_fr.fit_transform(fr_emb.vectors)"
      ],
      "metadata": {
        "id": "LhgAjrscbBQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fr_emb.vectors = vectors_fr"
      ],
      "metadata": {
        "id": "vWZOGETab0nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca_eng = PCA(n_components=100)\n",
        "vectors_eng = pca_eng.fit_transform(fr_emb.vectors)"
      ],
      "metadata": {
        "id": "KrMx5P1mcg64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_emb.vectors=vectors_eng"
      ],
      "metadata": {
        "id": "zn3cYtqycjp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_pca(sentence):\n",
        "    translated = []\n",
        "\n",
        "    tk = WordPunctTokenizer()\n",
        "\n",
        "    for word in tk.tokenize(sentence):\n",
        "        try:\n",
        "          translated.append(fr_emb.most_similar([np.matmul(en_emb[word], mapping_svd_pca)])[0][0])\n",
        "          #translated.append(fr_emb.most_similar(mapping.predict(en_emb[word].reshape(1, -1)))[0][0])\n",
        "        except KeyError:\n",
        "          continue\n",
        "\n",
        "    return \" \".join(translated)"
      ],
      "metadata": {
        "id": "B6mx4Uf6bd9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translations_pca = [translate_pca(tweet) for tweet in processed_tweets]\n",
        "mean_f_measure(translations_pca, bing_translations)"
      ],
      "metadata": {
        "id": "UF4QZlrtbzd1",
        "outputId": "d1c3e20d-33e2-4e92-93e5-a38057d4167f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'processed_tweets' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-55d9ef576368>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtranslations_pca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtranslate_pca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessed_tweets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'processed_tweets' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well, all these ways are not quite effective, as I think that the problem is in the initial way of mapping. I researched a lot of articles and I suppose another ways to drastically improve mapping quality would be ISF, CCA (I tried to implement it but failed, did something wrong and got 0.08 f_measure), RCSLS - these are the most populars ways to make this model work, but I suppose their implementation would take more resources and knowledge.\n",
        "\n",
        "\n",
        "Also, it would be great to implement FastText vectors (with full n-gramms) to our model, as it would allow us to decypher all OOV words, especially slang and hashtags, that we met in tweeter dataset."
      ],
      "metadata": {
        "id": "6hevX7W-d6Zz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXMxWUtipDD8"
      },
      "source": [
        "Great!"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
