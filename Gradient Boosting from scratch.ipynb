{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "id": "GOqjUI6igeLc"
   },
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для оптимизации значений гаммы\n",
    "\n",
    "from scipy.optimize import minimize_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "id": "ZB5Yt-LKgeLi"
   },
   "outputs": [],
   "source": [
    "class GradientBoosting:\n",
    "    def __init__(\n",
    "        self, \n",
    "        base_model_class: object = DecisionTreeRegressor,\n",
    "        base_model_params: dict = {'max_depth': None}, \n",
    "        n_estimators: int = 10,\n",
    "        learning_rate: float = 0.1\n",
    "    ):\n",
    "        \n",
    "        \n",
    "        self.base_model_class = base_model_class\n",
    "        self.base_model_params = base_model_params\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # list for optimal gammas at each iteration\n",
    "        self.gammas = []\n",
    "        \n",
    "        # list for base models\n",
    "        self.models = []\n",
    "        \n",
    "        \n",
    "        # Я создала 2 новые функции: одна для подсчета градиента (просто ради удобства),\n",
    "        # Вторая для подсчета функции потерь с учетом гаммы (ее я потом оптимизировала в функции find_optimal_gamma)\n",
    "        \n",
    "    def gradient_calc(self, y_true, y_pred, n_estimators):\n",
    "    \n",
    "        grad = (-2/self.n_estimators)*(y_true-y_pred)\n",
    "    \n",
    "        return grad\n",
    "    \n",
    "    def gamma_loss(self, y, y_pred_old, y_pred_new, gamma):\n",
    "        \n",
    "        return sum((1/self.n_estimators)*np.transpose((y_pred_old + gamma*(y_pred_new-y_pred_old) - y))*(y_pred_old + gamma*(y_pred_new-y_pred_old) - y))\n",
    "    \n",
    "    def find_optimal_gamma(self, y, y_pred_old, y_pred_new):\n",
    "        \n",
    "        min_loss = minimize_scalar(lambda gamma: self.gamma_loss(y, y_pred_old, y_pred_new, gamma),bounds=(0,1))\n",
    "        \n",
    "        return min_loss.x\n",
    "    \n",
    "    \n",
    "    def _fit_base_model(self, X: np.ndarray, y: np.array):\n",
    "        \n",
    "        model = self.base_model_class(**self.base_model_params)\n",
    "        model.fit(X,y)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "        \n",
    "    def fit(self, X: np.ndarray, y: np.array):\n",
    "        \n",
    "        # Отдельно обучила первую модель из композиции, задала первый прогноз ее прогнозом\n",
    "        \n",
    "        initial_pred_model = self.base_model_class(**self.base_model_params)\n",
    "        initial_pred = initial_pred_model.fit(X,y)\n",
    "        \n",
    "        y_pred = initial_pred_model.predict(X)\n",
    "        \n",
    "        self.models.append(initial_pred)\n",
    "        \n",
    "        # Итеративно обучила каждую модель композиции на подвыборках градиентов соответствующих данных\n",
    "\n",
    "        for iteration in range(self.n_estimators): \n",
    "            \n",
    "\n",
    "            grad = self.gradient_calc(y, y_pred, self.n_estimators)\n",
    "            \n",
    "            X_k, grad_k = resample(X, grad, n_samples = len(X)//(self.n_estimators))\n",
    "        \n",
    "            model = self._fit_base_model(X_k, grad_k)\n",
    "        \n",
    "            pred_grad = model.predict(X)\n",
    "            \n",
    "            # Для оптимизации гаммы я отдельно сохраняла старые и новые прогнозы (без гаммы)\n",
    "            \n",
    "            y_pred_old = y_pred\n",
    "            y_pred_new = y_pred + self.learning_rate*pred_grad\n",
    "            \n",
    "            # Потом использовала их для оптимизации как аргументы функции\n",
    "            \n",
    "            \n",
    "            gamma = self.find_optimal_gamma(y, y_pred_old, y_pred_new)\n",
    "            \n",
    "            self.gammas.append(gamma)\n",
    "            \n",
    "            # Полноценно обновляла прогноз через новые предсказания (уже с гаммой)\n",
    "            \n",
    "            y_pred += gamma*self.learning_rate * pred_grad\n",
    "            \n",
    "            self.models.append(model)\n",
    "        \n",
    "        return self.models\n",
    "        \n",
    "       \n",
    "        \n",
    "    def predict(self, X: np.ndarray):\n",
    "        \n",
    "        # Изначальное предсказание - просто предсказание первого дерева\n",
    "        \n",
    "        final_pred = self.models[0].predict(X)\n",
    "        \n",
    "        # Потом итеративно проходимся по всем деревьям, суммируем предсказания\n",
    "    \n",
    "        for iteration in range(1,len(self.models)):\n",
    "        \n",
    "            model = self.models[iteration]\n",
    "            \n",
    "            temp_pred = model.predict(X)\n",
    "            \n",
    "            final_pred += self.learning_rate*temp_pred\n",
    "    \n",
    "        return final_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверьте вашу реализацию на бостонском датасете. Подберите оптимальные гиперпараметры, чтобы победить RandomForestRegressor (не меняйте параметры сида)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "X = housing.data[:5000]\n",
    "y = housing.target[:5000]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1673784076585523"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(max_features=4, n_estimators=640, random_state=19052019)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "mean_squared_error(y_test, rf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33684880318576"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GB = GradientBoosting()\n",
    "GB.fit(X_train, y_train)\n",
    "mean_squared_error(y_test, GB.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24077874106988897 0.003 27 7\n"
     ]
    }
   ],
   "source": [
    "# Попробуем подобрать гиперпараметры (это окошко можно не запускать, я делала его для своего удобства)\n",
    "\n",
    "\n",
    "final_error = 0.4\n",
    "for lr in range(100, 10, -5):\n",
    "    for n in range (2, 30, 1):\n",
    "        for depth in range(6,10):\n",
    "            GB = GradientBoosting(learning_rate = lr/10000, n_estimators = n, base_model_params = {'max_depth': depth})\n",
    "            GB.fit(X_train, y_train)\n",
    "            current_error = mean_squared_error(y_test, GB.predict(X_test))\n",
    "            if final_error > current_error:\n",
    "                final_error = current_error\n",
    "                optimal_lr = lr/10000\n",
    "                optimal_n = n\n",
    "                optimal_depth = depth\n",
    "print(final_error, optimal_lr, optimal_n, optimal_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Победить случайный лес у меня так и не получилось, вышло только приблизить\n",
    "# значение ошибки к 0.167 - минимальная ошибка на кастомном бустинге при подборе параметров это примерно 0.24\n",
    "# этого вышло добиться с гиперпараметрами: lr = 0.003, n_estimators = 27, max_depth = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24927436890619278\n"
     ]
    }
   ],
   "source": [
    "GB = GradientBoosting(learning_rate = 0.003, n_estimators = 27, base_model_params = {'max_depth': 7})\n",
    "GB.fit(X_train, y_train)\n",
    "print(mean_squared_error(y_test, GB.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0161254278818677"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Еще ради интереса провела эксперимент с xgboost \n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "model = XGBRegressor()\n",
    "model.fit(X, y)\n",
    "mean_squared_error(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравниваем подходы\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(34316) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "!wget  -O 'bank_data.csv' -q 'https://www.dropbox.com/s/uy27mctxo0gbuof/bank_data.csv?dl=0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('bank_data.csv')\n",
    "df.sample(5)\n",
    "\n",
    "f_names = ['age', 'job', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Решите задачу предсказания возвращения кредита методами, перечисленными ниже:\n",
    "\n",
    "- Случайный лес\n",
    "- Бэггинг на деревьях (поставьте для базовых деревьев min_samples_leaf=1)\n",
    "- Бэггинг, у которого базовой моделью является бустинг с большим числом деревьев (> 100)\n",
    "- Бэггинг на логистических регрессиях\n",
    "\n",
    "Используйте логистическую регрессию, случайный лес, `GradientBoostingClassifier` и `BaggingClassifier` из `sklearn`.\n",
    "\n",
    "1) Какая из моделей имеет лучшее качество? С чем это связано?\n",
    "\n",
    "2) Какая из моделей сильнее всего переобучается?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df[f_names].values\n",
    "\n",
    "y_nr = 1 * (df['y'].values)\n",
    "\n",
    "y = y_nr.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "enc = OrdinalEncoder()\n",
    "\n",
    "X_enc = enc.fit_transform(X)\n",
    "y_enc = enc.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_enc, y_enc, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.759129297313985"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Случайный лес\n",
    "    \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(X_train, np.ravel(y_train))\n",
    "log_loss(np.ravel(y_test), rf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.4104493282649555"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Бэггинг на деревьях (min_samples_leaf = 1)\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bc_dt = BaggingClassifier(base_estimator=DecisionTreeClassifier(min_samples_leaf = 1))\n",
    "\n",
    "bc_dt.fit(X_train, np.ravel(y_train))\n",
    "log_loss(np.ravel(y_test), bc_dt.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7963469447250633"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Бэггинг, у которого базовой моделью является бустинг с большим числом деревьев (> 100)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "bc_gb = BaggingClassifier(base_estimator=GradientBoostingClassifier(n_estimators = 101))\n",
    "\n",
    "bc_gb.fit(X_train, np.ravel(y_train))\n",
    "log_loss(np.ravel(y_test), bc_gb.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5779352038785195"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Бэггинг на логистических регрессиях (увеличила количество итераций из-за рекомендации предупреждения*)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "bc_lr = BaggingClassifier(base_estimator=LogisticRegression(max_iter=1000))\n",
    "\n",
    "bc_lr.fit(X_train, np.ravel(y_train))\n",
    "log_loss(np.ravel(y_test), bc_lr.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ошибка на тренировочной выборке:\n",
      "9.992007221626413e-16\n",
      "Ошибка на тестовой выборке:\n",
      "3.759129297313985\n"
     ]
    }
   ],
   "source": [
    "# Проверка переобучения:\n",
    "\n",
    "# 1. Случайный лес\n",
    "\n",
    "\n",
    "print(\"Ошибка на тренировочной выборке:\")\n",
    "print(log_loss(np.ravel(y_train), rf.predict(X_train)))\n",
    "print(\"Ошибка на тестовой выборке:\")\n",
    "print(log_loss(np.ravel(y_test), rf.predict(X_test)))\n",
    "\n",
    "# Переобучение есть, и оно сильное"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ошибка на тренировочной выборке:\n",
      "0.2791416181610766\n",
      "Ошибка на тестовой выборке:\n",
      "4.4104493282649555\n"
     ]
    }
   ],
   "source": [
    "# 2. Бэггинг на деревьях\n",
    "\n",
    "print(\"Ошибка на тренировочной выборке:\")\n",
    "print(log_loss(np.ravel(y_train), bc_dt.predict(X_train)))\n",
    "print(\"Ошибка на тестовой выборке:\")\n",
    "print(log_loss(np.ravel(y_test), bc_dt.predict(X_test)))\n",
    "\n",
    "# Переобучение есть, но чуть слабее, чем у случайного леса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ошибка на тренировочной выборке:\n",
      "3.6056004702971554\n",
      "Ошибка на тестовой выборке:\n",
      "3.7963469447250633\n"
     ]
    }
   ],
   "source": [
    "# 3. Бэггинг на бустингах деревьев\n",
    "\n",
    "print(\"Ошибка на тренировочной выборке:\")\n",
    "print(log_loss(np.ravel(y_train), bc_gb.predict(X_train)))\n",
    "print(\"Ошибка на тестовой выборке:\")\n",
    "print(log_loss(np.ravel(y_test), bc_gb.predict(X_test)))\n",
    "\n",
    "# Переобучения нет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ошибка на тренировочной выборке:\n",
      "4.69889471205243\n",
      "Ошибка на тестовой выборке:\n",
      "4.5779352038785195\n"
     ]
    }
   ],
   "source": [
    "# 4. Бэггинг на логистических регрессиях\n",
    "\n",
    "print(\"Ошибка на тренировочной выборке:\")\n",
    "print(log_loss(np.ravel(y_train), bc_lr.predict(X_train)))\n",
    "print(\"Ошибка на тестовой выборке:\")\n",
    "print(log_loss(np.ravel(y_test), bc_lr.predict(X_test)))\n",
    "\n",
    "# Переобучения нет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Лучше всего показывает себя Случайный лес.\n",
    "\n",
    "Это происходит потому, что он учится на подвыборках, то есть вынужденно добавляет разнообразия и \"случайности\".\n",
    "Деревья получаются независимыми друг от друга, в отличие от моделей бэггинга.\n",
    "В данной задаче классификации это позволяет учитывать все признаки, не привязывать их друг к другу, учитывать все\n",
    "критерии, которые могут влиять на результирующий таргет.\n",
    "\n",
    "Сильнее всего переобучается Случайный лес.\n",
    "\n",
    "Возможно, по умолчанию модель слишком сложна для этой задачи, и деревья чересчур глубокие,\n",
    "подстраиваются под тренировочную выборку слишком сильно. К тому же в дефолтной модели у нас не оптимизрованы\n",
    "и другие гиперпараметры, это так же может влиять\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравнение встроенного бустинга xgboos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(34379) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): - ^C\n",
      "failed\n",
      "\n",
      "CondaError: KeyboardInterrupt\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#conda install -c conda-forge xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7777325203909715"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "model = XGBClassifier(eval_metric='logloss')\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "log_loss(y_test, model.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'max_depth': range (2, 10, 1),\n",
    "    'n_estimators': range(60, 220, 40),\n",
    "    'learning_rate': [0.1, 0.01, 0.05]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(model, parameters, scoring='neg_log_loss')\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print((-1)*clf.best_score_, clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB!: У нас выходит отрицательная логистическая функция потерь, но это нормально, тк такова особенность\n",
    "# работы АПИ скоринга Sklearn, он по умолчанию нацелен на максимизацию функционала качества, и в случае,\n",
    "# где нужно минимизировать функционал, он домножает его на -1. Истинное значение логистической функции потерь\n",
    "# это просто модуль получившегося значения. То есть ошибка равна примерно 0.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ОТВЕТЫ НА ВОПРОСЫ:\n",
    "\n",
    "Получилось явно гораздо лучше, чем с моделями выше. LogLoss = 3.6 vs LogLoss = 0.27 !\n",
    "\n",
    "Потому что XGBoost итеративно оптимизирует гиперпараметры с каждым шагом, лучше справляется с отсутствием\n",
    "баланса в классах и учится на собственных ошибках предыдущей итерации на каждом шаге.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw8-boosting-clustering.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
